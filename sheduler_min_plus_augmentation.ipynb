{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Распознавание текста\n\n## CRNN+CTC loss baseline\n\nВ данном ноутбуке представлен baseline модели распознавания текста с помощью CRNN модели и CTC loss. Вы можете добавить новые аугментации или изменить структуру данной модели, или же попробовать совершенно новую архитектуру.","metadata":{}},{"cell_type":"markdown","source":"# 0. Установка и подгрузука библиотек","metadata":{}},{"cell_type":"markdown","source":"Установка библиотек, под которым запускается данный бейзлайн.","metadata":{}},{"cell_type":"code","source":"!pip install numpy==1.20.3\n!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n!pip install opencv-python==4.5.2.52\n!pip install matplotlib==3.4.2","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:29.686714Z","iopub.execute_input":"2022-03-01T16:03:29.687076Z","iopub.status.idle":"2022-03-01T16:03:53.172153Z","shell.execute_reply.started":"2022-03-01T16:03:29.687032Z","shell.execute_reply":"2022-03-01T16:03:53.171289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nimport numpy as np\nimport cv2\nimport os\nimport json\nfrom PIL import Image\nfrom matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:53.175974Z","iopub.execute_input":"2022-03-01T16:03:53.176218Z","iopub.status.idle":"2022-03-01T16:03:53.181711Z","shell.execute_reply.started":"2022-03-01T16:03:53.176190Z","shell.execute_reply":"2022-03-01T16:03:53.180703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Разделим трейн датасет на обучающую и валидационную подвыборки\n","metadata":{}},{"cell_type":"markdown","source":"Сначала преобразуем таблицу (в которой есть колонка base_image) в `labels.json` - это формат из второго этапа олимпиады, для которого составлялся бейзлайн. По сути это просто словарь из колонок 'file_name' и 'text'.","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:53.183631Z","iopub.execute_input":"2022-03-01T16:03:53.183888Z","iopub.status.idle":"2022-03-01T16:03:53.193736Z","shell.execute_reply.started":"2022-03-01T16:03:53.183854Z","shell.execute_reply":"2022-03-01T16:03:53.193017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv('../input/tryyyy/data/data/train_recognition/labels.csv')\ntrain_csv = train_csv.sample(frac = 1)\n\ntrain_data = dict(train_csv[['file_name','text']].values)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:53.194881Z","iopub.execute_input":"2022-03-01T16:03:53.195623Z","iopub.status.idle":"2022-03-01T16:03:53.689708Z","shell.execute_reply.started":"2022-03-01T16:03:53.195585Z","shell.execute_reply":"2022-03-01T16:03:53.688939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = [(k, v) for k, v in train_data.items()]\nprint('train len', len(train_data))\n\nsplit_coef = 0.75\ntrain_len = int(len(train_data)*split_coef)\n\ntrain_data_splitted = train_data[:train_len]\nval_data_splitted = train_data[train_len:]\n\nprint('train len after split', len(train_data_splitted))\nprint('val len after split', len(val_data_splitted))","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:53.692177Z","iopub.execute_input":"2022-03-01T16:03:53.692438Z","iopub.status.idle":"2022-03-01T16:03:53.766566Z","shell.execute_reply.started":"2022-03-01T16:03:53.692404Z","shell.execute_reply":"2022-03-01T16:03:53.765747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"./data\",exist_ok=True)\nos.makedirs(\"./data/train_recognition\",exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:53.767953Z","iopub.execute_input":"2022-03-01T16:03:53.768405Z","iopub.status.idle":"2022-03-01T16:03:53.773158Z","shell.execute_reply.started":"2022-03-01T16:03:53.768356Z","shell.execute_reply":"2022-03-01T16:03:53.772375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./data/train_recognition/train_labels_splitted.json', 'w') as f:\n    json.dump(dict(train_data_splitted), f)\n    \nwith open('./data/train_recognition/val_labels_splitted.json', 'w') as f:\n    json.dump(dict(val_data_splitted), f)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:53.774668Z","iopub.execute_input":"2022-03-01T16:03:53.774970Z","iopub.status.idle":"2022-03-01T16:03:54.122493Z","shell.execute_reply.started":"2022-03-01T16:03:53.774932Z","shell.execute_reply":"2022-03-01T16:03:54.121674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Зададим параметры обучения\n\nЗдесь мы можем поправить конфиги обучения - задать размер батча, количество эпох, размер входных изображений, а также установить пути к датасетам.","metadata":{}},{"cell_type":"code","source":"alphabet = ''.join(sorted(list(set(''.join(train_csv['text'].values)))))","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:54.123917Z","iopub.execute_input":"2022-03-01T16:03:54.124173Z","iopub.status.idle":"2022-03-01T16:03:54.190009Z","shell.execute_reply.started":"2022-03-01T16:03:54.124124Z","shell.execute_reply":"2022-03-01T16:03:54.188857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alphabet","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:54.191693Z","iopub.execute_input":"2022-03-01T16:03:54.192070Z","iopub.status.idle":"2022-03-01T16:03:54.202181Z","shell.execute_reply.started":"2022-03-01T16:03:54.192031Z","shell.execute_reply":"2022-03-01T16:03:54.201327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nconfig_json = {\n    \"alphabet\": ''' !\"%\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№''',\n    \"save_dir\": \"./experiments/test\",\n    \"num_epochs\": 100,\n    \"image\": {\n        \"width\": 256,\n        \"height\": 64\n    },\n    \"train\": {\n        \"root_path\": \"../input/tryyyy/data/data/train_recognition/images\",\n        \"json_path\": \"./data/train_recognition/train_labels_splitted.json\",\n        \"batch_size\": 216\n    },\n    \"val\": {\n        \"root_path\": \"../input/tryyyy/data/data/train_recognition/images\",\n        \"json_path\": \"./data/train_recognition/val_labels_splitted.json\",\n        \"batch_size\": 216\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:54.203539Z","iopub.execute_input":"2022-03-01T16:03:54.204007Z","iopub.status.idle":"2022-03-01T16:03:54.213093Z","shell.execute_reply.started":"2022-03-01T16:03:54.203967Z","shell.execute_reply":"2022-03-01T16:03:54.212265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open(config_json['train']['json_path']) as json_file:\n#     data = json.load(json_file)\n# images = list(data.keys())\n# image = cv2.imread(f\"{config_json['train']['root_path']}/{images[randint(0, len(images))]}\")\n# plt.imshow(image)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:54.214505Z","iopub.execute_input":"2022-03-01T16:03:54.214768Z","iopub.status.idle":"2022-03-01T16:03:54.225787Z","shell.execute_reply.started":"2022-03-01T16:03:54.214734Z","shell.execute_reply":"2022-03-01T16:03:54.225076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:54.228496Z","iopub.execute_input":"2022-03-01T16:03:54.228692Z","iopub.status.idle":"2022-03-01T16:03:54.237111Z","shell.execute_reply.started":"2022-03-01T16:03:54.228669Z","shell.execute_reply":"2022-03-01T16:03:54.236435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Теперь определим класс датасета (torch.utils.data.Dataset) и другие вспомогательные функции","metadata":{}},{"cell_type":"code","source":"!pip uninstall opencv-python -y\n!pip install opencv-python==4.5.2.52\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:03:54.238428Z","iopub.execute_input":"2022-03-01T16:03:54.238842Z","iopub.status.idle":"2022-03-01T16:04:05.819283Z","shell.execute_reply.started":"2022-03-01T16:03:54.238805Z","shell.execute_reply":"2022-03-01T16:04:05.818428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# функция которая помогает объединять картинки и таргет-текст в батч\ndef collate_fn(batch):\n    images, texts, enc_texts = zip(*batch)\n    images = torch.stack(images, 0)\n    text_lens = torch.LongTensor([len(text) for text in texts])\n    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n    return images, texts, enc_pad_texts, text_lens\n\n\ndef get_data_loader(\n    transforms, json_path, root_path, tokenizer, batch_size, drop_last\n):\n    dataset = OCRDataset(json_path, root_path, tokenizer, transforms)\n    data_loader = torch.utils.data.DataLoader(\n        dataset=dataset,\n        collate_fn=collate_fn,\n        batch_size=batch_size,\n        num_workers=8,\n    )\n    return data_loader\n\n\nclass OCRDataset(Dataset):\n    def __init__(self, json_path, root_path, tokenizer, transform=None):\n        super().__init__()\n        self.transform = transform\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n        self.data_len = len(data)\n\n        self.img_paths = []\n        self.texts = []\n        for img_name, text in data.items():\n            self.img_paths.append(os.path.join(root_path, img_name))\n            self.texts.append(text)\n        self.enc_texts = tokenizer.encode(self.texts)\n\n    def __len__(self):\n        return self.data_len\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        text = self.texts[idx]\n        enc_text = torch.LongTensor(self.enc_texts[idx])\n        \n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n            \n        return image, text, enc_text\n\n\nclass AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:05.824630Z","iopub.execute_input":"2022-03-01T16:04:05.824849Z","iopub.status.idle":"2022-03-01T16:04:05.844017Z","shell.execute_reply.started":"2022-03-01T16:04:05.824824Z","shell.execute_reply":"2022-03-01T16:04:05.843307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Здесь определен Токенайзер - вспопогательный класс, который преобразует текст в числа\n\nРазметка-текст с картинок преобразуется в числовое представление, на которых модель может учиться. Также может преобразовывать числовое предсказание модели обратно в текст.","metadata":{}},{"cell_type":"code","source":"OOV_TOKEN = '<OOV>'\nCTC_BLANK = '<BLANK>'\n\n\ndef get_char_map(alphabet):\n    \"\"\"Make from string alphabet character2int dict.\n    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n    char_map[CTC_BLANK] = 0\n    char_map[OOV_TOKEN] = 1\n    return char_map\n\n\nclass Tokenizer:\n    \"\"\"Class for encoding and decoding string word to sequence of int\n    (and vice versa) using alphabet.\"\"\"\n\n    def __init__(self, alphabet):\n        self.char_map = get_char_map(alphabet)\n        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n\n    def encode(self, word_list):\n        \"\"\"Returns a list of encoded words (int).\"\"\"\n        enc_words = []\n        for word in word_list:\n            enc_words.append(\n                [self.char_map[char] if char in self.char_map\n                 else self.char_map[OOV_TOKEN]\n                 for char in word]\n            )\n        return enc_words\n\n    def get_num_chars(self):\n        return len(self.char_map)\n\n    def decode(self, enc_word_list):\n        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n        repeating characters. Also skip out of vocabulary token.\"\"\"\n        dec_words = []\n        for word in enc_word_list:\n            word_chars = ''\n            for idx, char_enc in enumerate(word):\n                # skip if blank symbol, oov token or repeated characters\n                if (\n                    char_enc != self.char_map[OOV_TOKEN]\n                    and char_enc != self.char_map[CTC_BLANK]\n                    # idx > 0 to avoid selecting [-1] item\n                    and not (idx > 0 and char_enc == word[idx - 1])\n                ):\n                    word_chars += self.rev_char_map[char_enc]\n            dec_words.append(word_chars)\n        return dec_words","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:05.845537Z","iopub.execute_input":"2022-03-01T16:04:05.845919Z","iopub.status.idle":"2022-03-01T16:04:05.860763Z","shell.execute_reply.started":"2022-03-01T16:04:05.845872Z","shell.execute_reply":"2022-03-01T16:04:05.859842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Accuracy в качестве метрики\n\nAccuracy измеряет долю предсказанных строк текста, которые полностью совпадают с таргет текстом.","metadata":{}},{"cell_type":"code","source":"def get_accuracy(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        scores.append(true == pred)\n    avg_score = np.mean(scores)\n    return avg_score","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:05.863376Z","iopub.execute_input":"2022-03-01T16:04:05.863685Z","iopub.status.idle":"2022-03-01T16:04:05.874640Z","shell.execute_reply.started":"2022-03-01T16:04:05.863634Z","shell.execute_reply":"2022-03-01T16:04:05.873794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Аугментации\n\nЗдесь мы задаем базовые аугментации для модели. Вы можете написать свои или использовать готовые библиотеки типа albumentations","metadata":{}},{"cell_type":"code","source":"!pip install albumentations","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:05.876059Z","iopub.execute_input":"2022-03-01T16:04:05.876787Z","iopub.status.idle":"2022-03-01T16:04:13.408292Z","shell.execute_reply.started":"2022-03-01T16:04:05.876748Z","shell.execute_reply":"2022-03-01T16:04:13.407360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n#from albumentations.pytorch.transforms import ToTensorV2\n\nimport random\nimport copy\n\nheight = config_json['image']['height']\nwidth = config_json['image']['width']\n\nclass ToTensor:\n    def __call__(self, arr):\n        arr = torch.from_numpy(arr)\n        return arr\n\nclass Normalize:\n    def __call__(self, image):\n        image = image.astype(np.float32) / 255\n        return image\n\n\nclass MoveChannels:\n    \"\"\"Move the channel axis to the zero position as required in pytorch.\"\"\"\n    def __init__(self, to_channels_first=True):\n        self.to_channels_first = to_channels_first\n\n    def __call__(self, image):\n        if self.to_channels_first:\n            return np.moveaxis(image, -1, 0)\n        else:\n            return np.moveaxis(image, 0, -1)\n\n\ntrain_transform = A.Compose([\n    A.augmentations.geometric.resize.Resize(height, width),\n    # MoveChannels(to_channels_first=True),\n    A.Perspective(scale=(0.025, 0.075), p=0.1),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.05, rotate_limit=[-5, 5], p=0.2),\n    A.RandomBrightnessContrast(p=0.2),\n    A.RandomContrast(p=0.2),\n    A.CLAHE(p=0.2),\n    A.Cutout(num_holes=10, max_h_size=4, max_w_size=4, fill_value=0, p=0.1),\n    A.Downscale(scale_min=0.9, scale_max=0.99,p=0.1),\n    A.Equalize(p=0.2),\n    A.Sharpen(alpha=(0.1, 0.4), lightness=(0.75, 1.0), p=0.2),\n    A.Normalize(mean=0, std=1),\n    ToTensorV2(),\n])\n\ntrain_dataset = OCRDataset(json_path=config_json['train']['json_path'],\n                           root_path=config_json['train']['root_path'],\n                           tokenizer=Tokenizer(config_json['alphabet']),\n                           transform=train_transform)\n\n\n\nval_transform = A.Compose([\n        A.augmentations.geometric.resize.Resize(height, width),\n        A.Normalize(mean=0, std=1),\n        ToTensorV2(),\n    ])\n\n\n\nval_dataset = OCRDataset(json_path=config_json['val']['json_path'],\n                           root_path=config_json['val']['root_path'],\n                           tokenizer=Tokenizer(config_json['alphabet']),\n                           transform=val_transform)\n\ndef visualize_augmentations(dataset, idx=0, samples=50, cols=10):\n    dataset = copy.deepcopy(dataset)\n    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n    rows = samples // cols\n    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n    for i in range(samples):\n        image= dataset[idx][0]\n        ax.ravel()[i].imshow(image)\n        ax.ravel()[i].set_axis_off()\n    plt.tight_layout()\n    plt.show()\n    \nvisualize_augmentations(train_dataset)\n\n\ndef get_train_transforms(height, width):\n    transform = A.Compose([\n    A.augmentations.geometric.resize.Resize(height, width),\n    # MoveChannels(to_channels_first=True),\n    A.Perspective(scale=(0.025, 0.075), p=0.1),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.05, rotate_limit=[-5, 5], p=0.2),\n    A.RandomBrightnessContrast(p=0.2),\n    A.RandomContrast(p=0.2),\n    A.CLAHE(p=0.2),\n    A.Cutout(num_holes=10, max_h_size=4, max_w_size=4, fill_value=0, p=0.1),\n    A.Downscale(scale_min=0.9, scale_max=0.99,p=0.1),\n    A.Equalize(p=0.2),\n    A.Sharpen(alpha=(0.1, 0.4), lightness=(0.75, 1.0), p=0.2),\n    A.Normalize(mean=0, std=1),\n    ToTensorV2(),\n])\n    return transform\n\n\n\n\n\ndef get_val_transforms(height, width):\n    transforms = A.Compose([\n        A.augmentations.geometric.resize.Resize(height, width),\n        A.Normalize(),\n        ToTensorV2(),\n    ])\n    return transforms","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:13.412223Z","iopub.execute_input":"2022-03-01T16:04:13.412468Z","iopub.status.idle":"2022-03-01T16:04:17.377474Z","shell.execute_reply.started":"2022-03-01T16:04:13.412439Z","shell.execute_reply":"2022-03-01T16:04:17.376777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Здесь определяем саму модель - CRNN\n\nПодробнее об архитектуре можно почитать в статье https://arxiv.org/abs/1507.05717","metadata":{}},{"cell_type":"code","source":"def get_resnet34_backbone(pretrained=True):\n    m = torchvision.models.resnet34(pretrained=True)\n    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n    blocks = [input_conv, m.bn1, m.relu,\n              m.maxpool, m.layer1, m.layer2, m.layer3]\n    return nn.Sequential(*blocks)\n\n\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size, hidden_size, num_layers,\n            dropout=dropout, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return out\n\n\nclass CRNN(nn.Module):\n    def __init__(\n        self, number_class_symbols, time_feature_count=256, lstm_hidden=256,\n        lstm_len=2,\n    ):\n        super().__init__()\n        self.feature_extractor = get_resnet34_backbone(pretrained=True)\n        self.avg_pool = nn.AdaptiveAvgPool2d(\n            (time_feature_count, time_feature_count))\n        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_hidden * 2, time_feature_count),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(time_feature_count, number_class_symbols)\n        )\n\n    def forward(self, x):\n        x = self.feature_extractor(x)\n        b, c, h, w = x.size()\n        x = x.view(b, c * h, w)\n        x = self.avg_pool(x)\n        x = x.transpose(1, 2)\n        x = self.bilstm(x)\n        x = self.classifier(x)\n        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:17.378681Z","iopub.execute_input":"2022-03-01T16:04:17.379415Z","iopub.status.idle":"2022-03-01T16:04:17.394025Z","shell.execute_reply.started":"2022-03-01T16:04:17.379375Z","shell.execute_reply":"2022-03-01T16:04:17.393159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Логирование лол","metadata":{}},{"cell_type":"code","source":"!pip install neptune-client","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:17.395648Z","iopub.execute_input":"2022-03-01T16:04:17.395908Z","iopub.status.idle":"2022-03-01T16:04:25.295127Z","shell.execute_reply.started":"2022-03-01T16:04:17.395872Z","shell.execute_reply":"2022-03-01T16:04:25.294306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Я тут логирование запилю, на всякий\n\nimport neptune.new as neptune\n\nrun = neptune.init(\n    project=\"temiusiii/OCR-NTO\",\n    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5NzBkMWVlMC1hMmI3LTQ3NDAtOTJjNy00NGMxOThhZTcyNWEifQ==\",\n)  # your credentials\n\nparams = {\n    \"alphabet\": config_json['alphabet'],\n    \"image_width\" : config_json['image']['width'],\n    \"image_heigth\" : config_json['image']['height']\n}\n\nrun[\"parameters\"] = params","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:25.297332Z","iopub.execute_input":"2022-03-01T16:04:25.298119Z","iopub.status.idle":"2022-03-01T16:04:29.930191Z","shell.execute_reply.started":"2022-03-01T16:04:25.298075Z","shell.execute_reply":"2022-03-01T16:04:29.929427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Переходим к самому скрипту обучения - циклы трейна и валидации","metadata":{}},{"cell_type":"code","source":"def val_loop(data_loader, model, tokenizer, device):\n    acc_avg = AverageMeter()\n    for images, texts, _, _ in data_loader:\n        batch_size = len(texts)\n        text_preds = predict(images, model, tokenizer, device)\n        acc_avg.update(get_accuracy(texts, text_preds), batch_size)\n    print(f'Validation, acc: {acc_avg.avg:.4f}')\n    return acc_avg.avg\n\n\ndef train_loop(data_loader, model, criterion, optimizer, epoch):\n    loss_avg = AverageMeter()\n    model.train()\n    for images, texts, enc_pad_texts, text_lens in data_loader:\n        model.zero_grad()\n        images = images.to(DEVICE)\n        batch_size = len(texts)\n        output = model(images)\n        output_lenghts = torch.full(\n            size=(output.size(1),),\n            fill_value=output.size(0),\n            dtype=torch.long\n        )\n        loss = criterion(output, enc_pad_texts, output_lenghts, text_lens)\n        loss_avg.update(loss.item(), batch_size)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n        optimizer.step()\n    for param_group in optimizer.param_groups:\n        lr = param_group['lr']\n    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}')\n    return loss_avg.avg\n\n\ndef predict(images, model, tokenizer, device):\n    model.eval()\n    images = images.to(device)\n    with torch.no_grad():\n        output = model(images)\n    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n    text_preds = tokenizer.decode(pred)\n    return text_preds\n\n\ndef get_loaders(tokenizer, config):\n    train_transforms = get_train_transforms(\n        height=config['image']['height'],\n        width=config['image']['width']\n    )\n    train_loader = get_data_loader(\n        json_path=config['train']['json_path'],\n        root_path=config['train']['root_path'],\n        transforms=train_transforms,\n        tokenizer=tokenizer,\n        batch_size=config['train']['batch_size'],\n        drop_last=True\n    )\n    val_transforms = get_val_transforms(\n        height=config['image']['height'],\n        width=config['image']['width']\n    )\n    val_loader = get_data_loader(\n        transforms=val_transforms,\n        json_path=config['val']['json_path'],\n        root_path=config['val']['root_path'],\n        tokenizer=tokenizer,\n        batch_size=config['val']['batch_size'],\n        drop_last=False\n    )\n    return train_loader, val_loader\n\n\ndef train(config):\n    tokenizer = Tokenizer(config['alphabet'])\n    os.makedirs(config['save_dir'], exist_ok=True)\n    train_loader, val_loader = get_loaders(tokenizer, config)\n\n    model = CRNN(number_class_symbols=tokenizer.get_num_chars())\n    model.to(DEVICE)\n\n    criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001,\n                                  weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer=optimizer, mode='min', factor=0.5, patience=15)\n    best_acc = -np.inf\n    acc_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n    for epoch in range(config['num_epochs']):\n        loss_avg = train_loop(train_loader, model, criterion, optimizer, epoch)\n        acc_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n        scheduler.step(acc_avg)\n        if acc_avg > best_acc:\n            best_acc = acc_avg\n            model_save_path = os.path.join(\n                config['save_dir'], f'model-{epoch}-{acc_avg:.4f}.ckpt')\n            torch.save(model.state_dict(), model_save_path)\n            print('Model weights saved')\n#----------------------------------------------------------------------------\n            run[\"train/accuracy\"].log(acc_avg)\n            run[\"train/loss\"].log(loss_avg)\n            run[\"train/epoch\"].log(epoch)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:29.931921Z","iopub.execute_input":"2022-03-01T16:04:29.932192Z","iopub.status.idle":"2022-03-01T16:04:29.953199Z","shell.execute_reply.started":"2022-03-01T16:04:29.932154Z","shell.execute_reply":"2022-03-01T16:04:29.952382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Запускаем обучение!","metadata":{}},{"cell_type":"code","source":"# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:29.955881Z","iopub.execute_input":"2022-03-01T16:04:29.956224Z","iopub.status.idle":"2022-03-01T16:04:29.966624Z","shell.execute_reply.started":"2022-03-01T16:04:29.956185Z","shell.execute_reply":"2022-03-01T16:04:29.965789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Если CUDA out of memory\n# import torch\n# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:04:29.967737Z","iopub.execute_input":"2022-03-01T16:04:29.968192Z","iopub.status.idle":"2022-03-01T16:04:29.976785Z","shell.execute_reply.started":"2022-03-01T16:04:29.968153Z","shell.execute_reply":"2022-03-01T16:04:29.976047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(config_json)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-01T16:04:29.977807Z","iopub.execute_input":"2022-03-01T16:04:29.979275Z","iopub.status.idle":"2022-03-01T18:56:31.396655Z","shell.execute_reply.started":"2022-03-01T16:04:29.979235Z","shell.execute_reply":"2022-03-01T18:56:31.395023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. Создание предсказаний для public-датасета\n\nСначала определим класс для создания предсказаний","metadata":{}},{"cell_type":"code","source":"class InferenceTransform:\n    def __init__(self, height, width):\n        self.transforms = get_val_transforms(height, width)\n\n    def __call__(self, images):\n        transformed_images = []\n        for image in images:\n            image = self.transforms(image)\n            transformed_images.append(image)\n        transformed_tensor = torch.stack(transformed_images, 0)\n        return transformed_tensor\n\n\nclass OcrPredictor:\n    def __init__(self, model_path, config, device='cuda'):\n        self.tokenizer = Tokenizer(config['alphabet'])\n        self.device = torch.device(device)\n        # load model\n        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.to(self.device)\n\n        self.transforms = InferenceTransform(\n            height=config['image']['height'],\n            width=config['image']['width'],\n        )\n\n    def __call__(self, images):\n        if isinstance(images, (list, tuple)):\n            one_image = False\n        elif isinstance(images, np.ndarray):\n            images = [images]\n            one_image = True\n        else:\n            raise Exception(f\"Input must contain np.ndarray, \"\n                            f\"tuple or list, found {type(images)}.\")\n\n        images = self.transforms(images)\n        pred = predict(images, self.model, self.tokenizer, self.device)\n\n        if one_image:\n            return pred[0]\n        else:\n            return pred","metadata":{"execution":{"iopub.status.busy":"2022-03-01T18:56:31.398069Z","iopub.status.idle":"2022-03-01T18:56:31.398495Z","shell.execute_reply.started":"2022-03-01T18:56:31.398265Z","shell.execute_reply":"2022-03-01T18:56:31.398287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Инициализируем OCR predictor","metadata":{}},{"cell_type":"code","source":"predictor = OcrPredictor(\n    model_path='experiments/test/model-99-0.7310.ckpt',\n    config=config_json\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T18:56:31.400001Z","iopub.status.idle":"2022-03-01T18:56:31.400466Z","shell.execute_reply.started":"2022-03-01T18:56:31.400210Z","shell.execute_reply":"2022-03-01T18:56:31.400235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим несколько предсказаний и создадим финальный json","metadata":{}},{"cell_type":"code","source":"pred_json = {}\n\nprint_images = True\nfor val_img in val_data_splitted[22:25]:\n    img = cv2.imread(f'train_recognition/images/{val_img[0]}')\n\n    pred = predictor(img)\n    pred_json[val_img[0]] = pred\n\n    if print_images:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.imshow(img)\n        plt.show()\n        print('Prediction: ', predictor(img))\n        print('True: ', val_img[1])\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-01T18:56:31.402072Z","iopub.status.idle":"2022-03-01T18:56:31.402520Z","shell.execute_reply.started":"2022-03-01T18:56:31.402283Z","shell.execute_reply":"2022-03-01T18:56:31.402306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сохраням submission json с предсказаниями","metadata":{}},{"cell_type":"code","source":"with open('prediction_HTR.json', 'w') as f:\n    json.dump(pred_json, f)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T18:56:31.403789Z","iopub.status.idle":"2022-03-01T18:56:31.404359Z","shell.execute_reply.started":"2022-03-01T18:56:31.404115Z","shell.execute_reply":"2022-03-01T18:56:31.404154Z"},"trusted":true},"execution_count":null,"outputs":[]}]}