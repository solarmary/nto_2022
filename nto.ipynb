{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Объединяем сегментацию и распознавание в одно работающее целое","metadata":{}},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:26:54.982281Z","iopub.execute_input":"2022-03-02T14:26:54.982628Z","iopub.status.idle":"2022-03-02T14:27:09.090881Z","shell.execute_reply.started":"2022-03-02T14:26:54.982557Z","shell.execute_reply":"2022-03-02T14:27:09.090106Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import sys\nimport os\nimport numpy as np\nimport random\n\nfrom tqdm import tqdm\nimport cv2\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport segmentation_models_pytorch as smp\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:28:29.068896Z","iopub.execute_input":"2022-03-02T14:28:29.069797Z","iopub.status.idle":"2022-03-02T14:28:34.551423Z","shell.execute_reply.started":"2022-03-02T14:28:29.069755Z","shell.execute_reply":"2022-03-02T14:28:34.550647Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"model\": {\n        \"name\": \"Unet\",\n        \"parameters\": {\n            \"encoder_name\": \"resnet34\",\n            \"encoder_weights\": None,\n            \"in_channels\": 3,\n            \"classes\": 1,\n        }\n    },\n    \"seed\": 42,\n    \"transforms\": A.Compose([\n        A.Resize(width=512, height=512),\n        A.Normalize(),\n        ToTensorV2(),\n    ]),\n\n    \"post-transforms\": A.Resize(width=3000, height=3000),\n    \"threshold\": 0.55,\n}\n\n#'../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:28:37.428297Z","iopub.execute_input":"2022-03-02T14:28:37.429066Z","iopub.status.idle":"2022-03-02T14:28:37.437841Z","shell.execute_reply.started":"2022-03-02T14:28:37.429031Z","shell.execute_reply":"2022-03-02T14:28:37.437123Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Predictor:\n    def __init__(self, model, device=\"cpu\", threshold=0.5):\n        self.model = model\n        self.device = torch.device(device)\n        self.threshold = threshold\n\n    def __call__(self, loader):\n        predictions = []\n\n        self.model.to(self.device)\n        with torch.no_grad():\n            for batch in tqdm(loader, position=0, leave=True, desc=\"Predicting: \"):\n                inputs = batch.to(self.device)\n                batch_predictions = self.model(inputs)\n                batch_predictions = batch_predictions.to(\"cpu\").detach()\n                batch_predictions = torch.where(batch_predictions > self.threshold, 1, 0)\n                batch_predictions = batch_predictions.permute(0, 2, 3, 1).numpy()\n                predictions.extend(batch_predictions)\n\n        predictions = np.array(predictions)\n        return predictions\n\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, pathes, masks=None, transforms=None):\n        self.pathes = pathes\n        self.masks = masks\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.pathes)\n\n    def __getitem__(self, index):\n        image_path = self.pathes[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.masks:\n            mask = self.masks[index].astype(np.int8)\n\n        if self.transforms:\n            if self.masks:\n                transformed = self.transforms(image=image, mask=mask)\n                image = transformed[\"image\"]\n                mask = transformed[\"mask\"]\n            else:\n                transformed = self.transforms(image=image)\n                image = transformed[\"image\"]\n\n        if self.masks:\n            return image, mask\n\n        return image\n\ndef main():\n    #test_image_path, output_path = sys.argv[1:]\n    test_image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    output_path = './try'\n    #test_images_filenames = os.listdir(test_image_path)\n    test_images_pathes = [test_image_path]\n    #test_images_pathes = [os.path.join(test_image_path, file) for file in test_images_filenames]\n    shapes = np.array([np.asarray(cv2.imread(path)).shape[1::-1] for path in test_images_pathes])\n    print(shapes)\n\n\n    dataset = SegmentationDataset(pathes=test_images_pathes, transforms=config[\"transforms\"])\n    loader = DataLoader(dataset=dataset,\n                        batch_size=9,\n                        shuffle=False,\n                        num_workers=0,\n                        pin_memory=True,\n                        drop_last=False)\n\n    print(f\"Loaded dataset with: {len(dataset)} samples\")\n\n    model_path = \"../input/tryyyy/weights/model-segmentation.pth\"\n    model = getattr(smp, config[\"model\"][\"name\"])(**config[\"model\"][\"parameters\"])\n    model_state = torch.load(model_path, map_location=torch.device('cpu'))[\"model_state\"]\n    model.load_state_dict(model_state)\n    print(f\"Loaded {model_path}\")\n    model.eval()\n\n    predictor = Predictor(model=model, device=\"cpu\", threshold=config[\"threshold\"])\n    predictions = predictor(loader)\n\n    resized_predictions = []\n    for prediction, shape in zip(predictions, shapes):\n        prediction = prediction.astype(np.int8)\n        resized_prediction = cv2.resize(prediction,shape, interpolation=cv2.INTER_NEAREST)\n        resized_predictions.append(resized_prediction)\n\n    resized_predictions = np.array(resized_predictions)\n    results = {k: v for k, v in zip(test_images_pathes, resized_predictions)}\n    np.savez_compressed(output_path, **results)\n\n    print(f\"Saved results: {output_path}\")\n    return results\n\n#хочу бви (-_-)\n\"\"\"if __name__ == \"__main__\":\n    image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    img = cv2.imread(image_path)\n    mask = main()\n    for contour in mask:\n        cv2.drawContours(img, np.array([contour]), -1, (0, 255, 0), 2)\n\n    plt.figure(figsize=(15, 15))\n    plt.imshow(img)\n    plt.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:41:39.724176Z","iopub.execute_input":"2022-03-02T14:41:39.724490Z","iopub.status.idle":"2022-03-02T14:41:39.752997Z","shell.execute_reply.started":"2022-03-02T14:41:39.724446Z","shell.execute_reply":"2022-03-02T14:41:39.750929Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_contours_from_mask(mask, min_area=5):\n    #mask = torch.from_numpy(mask)\n    contours, hierarchy = cv2.findContours(mask.astype(np.uint8),\n                                           cv2.RETR_LIST,\n                                           cv2.CHAIN_APPROX_SIMPLE)\n    contour_list = []\n    for contour in contours:\n        if cv2.contourArea(contour) >= min_area:\n            contour_list.append(contour)\n    return contour_list\n\n\nif __name__ == \"__main__\":\n    image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    img = cv2.imread(image_path)\n    mask = main()\n    contours = get_contours_from_mask(mask[image_path])\n    img = cv2.imread(image_path)\n\n    for contour in contours:\n        cv2.drawContours(img, np.array([contour]), -1, (0, 255, 0), 2)\n\n    plt.figure(figsize=(15, 15))\n    plt.imshow(img)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:43:16.109762Z","iopub.execute_input":"2022-03-02T14:43:16.110356Z","iopub.status.idle":"2022-03-02T14:43:19.826917Z","shell.execute_reply.started":"2022-03-02T14:43:16.110321Z","shell.execute_reply":"2022-03-02T14:43:19.826298Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_contours_from_mask(mask, min_area=5):\n    contours, hierarchy = cv2.findContours(mask.astype(np.uint8),\n                                           cv2.RETR_LIST,\n                                           cv2.CHAIN_APPROX_SIMPLE)\n    contour_list = []\n    for contour in contours:\n        if cv2.contourArea(contour) >= min_area:\n            contour_list.append(contour)\n    return contour_list\n\n\ndef get_larger_contour(contours):\n    larger_area = 0\n    larger_contour = None\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area > larger_area:\n            larger_contour = contour\n            larger_area = area\n    return larger_contour\n\ndef main():\n    #test_image_path, output_path = sys.argv[1:]\n    test_image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    output_path = './try'\n    #test_images_filenames = os.listdir(test_image_path)\n    test_images_pathes = [test_image_path]\n    #test_images_pathes = [os.path.join(test_image_path, file) for file in test_images_filenames]\n    shapes = np.array([np.asarray(cv2.imread(path)).shape[1::-1] for path in test_images_pathes])\n    print(shapes)\n\n    dataset = SegmentationDataset(pathes=test_images_pathes, transforms=config[\"transforms\"])\n    loader = DataLoader(dataset=dataset,\n                        batch_size=9,\n                        shuffle=False,\n                        num_workers=0,\n                        pin_memory=True,\n                        drop_last=False)\n\n    print(f\"Loaded dataset with: {len(dataset)} samples\")\n\n    model_path = \"../input/tryyyy/weights/model-segmentation.pth\"\n    model = getattr(smp, config[\"model\"][\"name\"])(**config[\"model\"][\"parameters\"])\n    model_state = torch.load(model_path, map_location=torch.device('cpu'))[\"model_state\"]\n    model.load_state_dict(model_state)\n    print(f\"Loaded {model_path}\")\n    model.eval()\n\n    predictor = Predictor(model=model, device=\"cpu\", threshold=config[\"threshold\"])\n    predictions = predictor(loader)\n\n    resized_predictions = []\n    for prediction, shape in zip(predictions, shapes):\n        prediction = prediction.astype(np.int8)\n        resized_prediction = cv2.resize(prediction,shape, interpolation=cv2.INTER_NEAREST)\n        resized_predictions.append(resized_prediction)\n\n    resized_predictions = np.array(resized_predictions)\n    \n    contours = []\n    for pred in prediction:\n            contour_list = get_contours_from_mask(pred)\n            contours.append(get_larger_contour(contour_list))\n    \n    #return contours\n    \n    #results = {k: v for k, v in zip(test_images_filenames, resized_predictions)}\n    results = {k: v for k, v in zip(test_images_pathes, resized_predictions)}\n    print(type(results))\n    print(results)\n    np.savez_compressed(output_path, **results)\n\n    print(f\"Saved results: {output_path}\")\n    return results\n\n#хочу бви (-_-)\nif __name__ == \"__main__\":\n    image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    img = cv2.imread(image_path)\n    mask = main()\n    for contour in mask:\n        cv2.drawContours(img, np.array([contour]), -1, (0, 255, 0), 2)\n\n    plt.figure(figsize=(15, 15))\n    plt.imshow(img)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T11:54:51.261259Z","iopub.execute_input":"2022-03-02T11:54:51.261516Z","iopub.status.idle":"2022-03-02T11:54:53.143974Z","shell.execute_reply.started":"2022-03-02T11:54:51.261487Z","shell.execute_reply":"2022-03-02T11:54:53.141923Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed:int=42) -> None:\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(config[\"seed\"])\n\n\nclass Predictor:\n    def __init__(self, model, device=\"cpu\", threshold=0.5):\n        self.model = model\n        self.device = torch.device(device)\n        self.threshold = threshold\n\n    def __call__(self, loader):\n        predictions = []\n\n        self.model.to(self.device)\n        with torch.no_grad():\n            for batch in tqdm(loader, position=0, leave=True, desc=\"Predicting: \"):\n                inputs = batch.to(self.device)\n                batch_predictions = self.model(inputs)\n                batch_predictions = batch_predictions.to(\"cpu\").detach()\n                batch_predictions = torch.where(batch_predictions > self.threshold, 1, 0)\n                batch_predictions = batch_predictions.permute(0, 2, 3, 1).numpy()\n                predictions.extend(batch_predictions)\n\n        predictions = np.array(predictions)\n        return predictions\n\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, pathes, masks=None, transforms=None):\n        self.pathes = pathes\n        self.masks = masks\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.pathes)\n\n    def __getitem__(self, index):\n        image_path = self.pathes[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.masks:\n            mask = self.masks[index].astype(np.int8)\n\n        if self.transforms:\n            if self.masks:\n                transformed = self.transforms(image=image, mask=mask)\n                image = transformed[\"image\"]\n                mask = transformed[\"mask\"]\n            else:\n                transformed = self.transforms(image=image)\n                image = transformed[\"image\"]\n\n        if self.masks:\n            return image, mask\n\n        return image\n\n\ndef main():\n    #test_image_path, output_path = sys.argv[1:]\n    test_image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    output_path = './try'\n    #test_images_filenames = os.listdir(test_image_path)\n    test_images_pathes = [test_image_path]\n    #test_images_pathes = [os.path.join(test_image_path, file) for file in test_images_filenames]\n    shapes = np.array([np.asarray(cv2.imread(path)).shape[1::-1] for path in test_images_pathes])\n    print(shapes)\n\n    dataset = SegmentationDataset(pathes=test_images_pathes, transforms=config[\"transforms\"])\n    loader = DataLoader(dataset=dataset,\n                        batch_size=9,\n                        shuffle=False,\n                        num_workers=0,\n                        pin_memory=True,\n                        drop_last=False)\n\n    print(f\"Loaded dataset with: {len(dataset)} samples\")\n\n    model_path = \"../input/tryyyy/weights/model-segmentation.pth\"\n    model = getattr(smp, config[\"model\"][\"name\"])(**config[\"model\"][\"parameters\"])\n    model_state = torch.load(model_path, map_location=torch.device('cpu'))[\"model_state\"]\n    model.load_state_dict(model_state)\n    print(f\"Loaded {model_path}\")\n    model.eval()\n\n    predictor = Predictor(model=model, device=\"cpu\", threshold=config[\"threshold\"])\n    predictions = predictor(loader)\n    \n   \n    \n    resized_predictions = []\n    for prediction, shape in zip(predictions, shapes):\n        prediction = prediction.astype(np.int8)\n        resized_prediction = cv2.resize(prediction,shape, interpolation=cv2.INTER_NEAREST)\n        resized_predictions.append(resized_prediction)\n\n    resized_predictions = np.array(resized_predictions)\n    \n    contours = []\n    for pred in resized_predictions:\n        contour_list = get_contours_from_mask(pred)\n        contours.append(get_larger_contour(contour_list))\n    \n    return contours\n\n    \"\"\"#results = {k: v for k, v in zip(test_images_filenames, resized_predictions)}\n    results = {k: v for k, v in zip(test_images_pathes, resized_predictions)}\n    np.savez_compressed(output_path, **results)\n\n    print(f\"Saved results: {output_path}\")\n    return results\"\"\"\n\n#хочу бви (-_-)\nif __name__ == \"__main__\":\n    image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    img = cv2.imread(image_path)\n    mask = main()\n    print(mask)\n    #mask = mask[list(mask.keys())[0]] у Вадима\n    \n    for contour in mask:\n        #print(type(contour))\n        cv2.drawContours(img, np.array([contour]), -1, (0, 255, 0), 2)\n\n        \nplt.figure(figsize=(15, 15))\nplt.imshow(img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T11:57:25.262743Z","iopub.execute_input":"2022-03-02T11:57:25.263038Z","iopub.status.idle":"2022-03-02T11:57:28.640198Z","shell.execute_reply.started":"2022-03-02T11:57:25.263008Z","shell.execute_reply":"2022-03-02T11:57:28.639532Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed:int=42) -> None:\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(config[\"seed\"])\n\n\nclass Predictor:\n    def __init__(self, model, device=\"cpu\", threshold=0.5):\n        self.model = model\n        self.device = torch.device(device)\n        self.threshold = threshold\n\n    def __call__(self, loader):\n        predictions = []\n\n        self.model.to(self.device)\n        with torch.no_grad():\n            for batch in tqdm(loader, position=0, leave=True, desc=\"Predicting: \"):\n                inputs = batch.to(self.device)\n                batch_predictions = self.model(inputs)\n                batch_predictions = batch_predictions.to(\"cpu\").detach()\n                batch_predictions = torch.where(batch_predictions > self.threshold, 1, 0)\n                batch_predictions = batch_predictions.permute(0, 2, 3, 1).numpy()\n                predictions.extend(batch_predictions)\n\n        predictions = np.array(predictions)\n        return predictions\n\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, pathes, masks=None, transforms=None):\n        self.pathes = pathes\n        self.masks = masks\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.pathes)\n\n    def __getitem__(self, index):\n        image_path = self.pathes[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.masks:\n            mask = self.masks[index].astype(np.int8)\n\n        if self.transforms:\n            if self.masks:\n                transformed = self.transforms(image=image, mask=mask)\n                image = transformed[\"image\"]\n                mask = transformed[\"mask\"]\n            else:\n                transformed = self.transforms(image=image)\n                image = transformed[\"image\"]\n\n        if self.masks:\n            return image, mask\n\n        return image\n\ndef get_contours_from_mask(mask, min_area=5):\n    contours, hierarchy = cv2.findContours(mask.astype(np.uint8),\n                                           cv2.RETR_LIST,\n                                           cv2.CHAIN_APPROX_SIMPLE)\n    contour_list = []\n    for contour in contours:\n        if cv2.contourArea(contour) >= min_area:\n            contour_list.append(contour)\n    return contour_list\n\n\ndef get_larger_contour(contours):\n    larger_area = 0\n    larger_contour = None\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area > larger_area:\n            larger_contour = contour\n            larger_area = area\n    return larger_contour\n    \ndef main():\n    #test_image_path, output_path = sys.argv[1:]\n    test_image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    output_path = './try'\n    #test_images_filenames = os.listdir(test_image_path)\n    test_images_pathes = [test_image_path]\n    #test_images_pathes = [os.path.join(test_image_path, file) for file in test_images_filenames]\n    shapes = np.array([np.asarray(cv2.imread(path)).shape[1::-1] for path in test_images_pathes])\n    print(shapes)\n\n    dataset = SegmentationDataset(pathes=test_images_pathes, transforms=config[\"transforms\"])\n    loader = DataLoader(dataset=dataset,\n                        batch_size=9,\n                        shuffle=False,\n                        num_workers=0,\n                        pin_memory=True,\n                        drop_last=False)\n\n    print(f\"Loaded dataset with: {len(dataset)} samples\")\n\n    model_path = \"../input/tryyyy/weights/model-segmentation.pth\"\n    model = getattr(smp, config[\"model\"][\"name\"])(**config[\"model\"][\"parameters\"])\n    model_state = torch.load(model_path, map_location=torch.device('cpu'))[\"model_state\"]\n    model.load_state_dict(model_state)\n    print(f\"Loaded {model_path}\")\n    model.eval()\n\n    predictor = Predictor(model=model, device=\"cpu\", threshold=config[\"threshold\"])\n    predictions = predictor(loader)\n\n    resized_predictions = []\n    for prediction, shape in zip(predictions, shapes):\n        prediction = prediction.astype(np.int8)\n        resized_prediction = cv2.resize(prediction,shape, interpolation=cv2.INTER_NEAREST)\n        resized_predictions.append(resized_prediction)\n\n    resized_predictions = np.array(resized_predictions)\n    results = {k: v for k, v in zip(test_images_pathes, resized_predictions)}\n    np.savez_compressed(output_path, **results)\n\n    print(f\"Saved results: {output_path}\")\n    \n    contours = []\n    for pred in resized_predictions:\n        contour_list = get_contours_from_mask(pred)\n        contours.append(get_larger_contour(contour_list))\n    \n    return [results, contours]\n\n\n#хочу бви (-_-)\n\nif __name__ == \"__main__\":    \n    image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    img = cv2.imread(image_path)\n    mask, contours = main()\n    \n    for contour in contours:\n        cv2.drawContours(img, np.array([contour]), -1, (0, 255, 0), 2)\n\n\n        \nplt.figure(figsize=(15, 15))\nplt.imshow(img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:35:52.529511Z","iopub.execute_input":"2022-03-02T09:35:52.529779Z","iopub.status.idle":"2022-03-02T09:35:56.136447Z","shell.execute_reply.started":"2022-03-02T09:35:52.529749Z","shell.execute_reply":"2022-03-02T09:35:56.135723Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Щас будем творить вторую часть OCR","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\n\nimport numpy as np\nimport cv2\nimport os\nimport sys\nimport json","metadata":{"execution":{"iopub.status.busy":"2022-03-02T10:29:10.604264Z","iopub.execute_input":"2022-03-02T10:29:10.604580Z","iopub.status.idle":"2022-03-02T10:29:12.357998Z","shell.execute_reply.started":"2022-03-02T10:29:10.604497Z","shell.execute_reply":"2022-03-02T10:29:12.357160Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"OOV_TOKEN = '<OOV>'\nCTC_BLANK = '<BLANK>'\n\n\ndef get_char_map(alphabet):\n    \"\"\"Make from string alphabet character2int dict.\n    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n    char_map[CTC_BLANK] = 0\n    char_map[OOV_TOKEN] = 1\n    return char_map\n\n\nclass Tokenizer:\n    \"\"\"Class for encoding and decoding string word to sequence of int\n    (and vice versa) using alphabet.\"\"\"\n\n    def __init__(self, alphabet):\n        self.char_map = get_char_map(alphabet)\n        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n\n    def encode(self, word_list):\n        \"\"\"Returns a list of encoded words (int).\"\"\"\n        enc_words = []\n        for word in word_list:\n            enc_words.append(\n                [self.char_map[char] if char in self.char_map\n                 else self.char_map[OOV_TOKEN]\n                 for char in word]\n            )\n        return enc_words\n\n    def get_num_chars(self):\n        return len(self.char_map)\n\n    def decode(self, enc_word_list):\n        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n        repeating characters. Also skip out of vocabulary token.\"\"\"\n        dec_words = []\n        for word in enc_word_list:\n            word_chars = ''\n            for idx, char_enc in enumerate(word):\n                # skip if blank symbol, oov token or repeated characters\n                if (\n                    char_enc != self.char_map[OOV_TOKEN]\n                    and char_enc != self.char_map[CTC_BLANK]\n                    # idx > 0 to avoid selecting [-1] item\n                    and not (idx > 0 and char_enc == word[idx - 1])\n                ):\n                    word_chars += self.rev_char_map[char_enc]\n            dec_words.append(word_chars)\n        return dec_words","metadata":{"execution":{"iopub.status.busy":"2022-03-02T10:47:50.096974Z","iopub.execute_input":"2022-03-02T10:47:50.097260Z","iopub.status.idle":"2022-03-02T10:47:50.107030Z","shell.execute_reply.started":"2022-03-02T10:47:50.097227Z","shell.execute_reply":"2022-03-02T10:47:50.106304Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Базовые трансформы модели\n\nЗдесь мы задаем базовые трансформы для инференса OCR","metadata":{}},{"cell_type":"code","source":"from albumentations.pytorch.transforms import ToTensorV2\n\nclass Normalize:\n    def __call__(self, img):\n        img = img.astype(np.float32) / 255\n        return img\n\n\n\n\nclass MoveChannels:\n    \"\"\"Move the channel axis to the zero position as required in pytorch.\"\"\"\n\n    def __init__(self, to_channels_first=True):\n        self.to_channels_first = to_channels_first\n\n    def __call__(self, image):\n        if self.to_channels_first:\n            return np.moveaxis(image, -1, 0)\n        else:\n            return np.moveaxis(image, 0, -1)\n\n\nclass ImageResize:\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n\n    def __call__(self, image):\n        image = cv2.resize(image, (self.width, self.height),\n                           interpolation=cv2.INTER_LINEAR)\n        return image\n\n\ndef get_val_transforms(height, width):\n    transforms = torchvision.transforms.Compose([\n        ImageResize(height, width),\n        MoveChannels(to_channels_first=True),\n        Normalize(),\n        ToTensorV2(),\n    ])\n    return transforms","metadata":{"execution":{"iopub.status.busy":"2022-03-02T10:47:51.951067Z","iopub.execute_input":"2022-03-02T10:47:51.951721Z","iopub.status.idle":"2022-03-02T10:47:51.960408Z","shell.execute_reply.started":"2022-03-02T10:47:51.951681Z","shell.execute_reply":"2022-03-02T10:47:51.959525Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### 2.3. Здесь определяем саму модель - CRNN","metadata":{}},{"cell_type":"code","source":"def get_resnet34_backbone(pretrained=True):\n    m = torchvision.models.resnet34(pretrained=True)\n    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n    blocks = [input_conv, m.bn1, m.relu,\n              m.maxpool, m.layer1, m.layer2, m.layer3]\n    return nn.Sequential(*blocks)\n\n\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size, hidden_size, num_layers,\n            dropout=dropout, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return out\n\n\nclass CRNN(nn.Module):\n    def __init__(\n        self, number_class_symbols, time_feature_count=256, lstm_hidden=256,\n        lstm_len=2,\n    ):\n        super().__init__()\n        self.feature_extractor = get_resnet34_backbone(pretrained=False)\n        self.avg_pool = nn.AdaptiveAvgPool2d(\n            (time_feature_count, time_feature_count))\n        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_hidden * 2, time_feature_count),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(time_feature_count, number_class_symbols)\n        )\n\n    def forward(self, x):\n        x = self.feature_extractor(x)\n        b, c, h, w = x.size()\n        x = x.view(b, c * h, w)\n        x = self.avg_pool(x)\n        x = x.transpose(1, 2)\n        x = self.bilstm(x)\n        x = self.classifier(x)\n        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-03-02T10:47:53.187252Z","iopub.execute_input":"2022-03-02T10:47:53.187640Z","iopub.status.idle":"2022-03-02T10:47:53.200113Z","shell.execute_reply.started":"2022-03-02T10:47:53.187605Z","shell.execute_reply":"2022-03-02T10:47:53.199210Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### 2.4. Определяем класс для использования OCR-модели на инференсе","metadata":{}},{"cell_type":"code","source":"def predict(images, model, tokenizer, device):\n    model.eval()\n    images = images.to(device)\n    with torch.no_grad():\n        output = model(images)\n    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n    text_preds = tokenizer.decode(pred)\n    return text_preds\n\n\nclass InferenceTransform:\n    def __init__(self, height, width):\n        self.transforms = get_val_transforms(height, width)\n\n    def __call__(self, images):\n        transformed_images = []\n        for image in images:\n            image = self.transforms(image)\n            transformed_images.append(image)\n        transformed_tensor = torch.stack(transformed_images, 0)\n        return transformed_tensor\n\n\nclass OcrPredictor:\n    def __init__(self, model_path, config, device='cuda'):\n        self.tokenizer = Tokenizer(config['alphabet'])\n        self.device = torch.device(device)\n        # load model\n        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.to(self.device)\n\n        self.transforms = InferenceTransform(\n            height=config['image']['height'],\n            width=config['image']['width'],\n        )\n\n    def __call__(self, images):\n        if isinstance(images, (list, tuple)):\n            one_image = False\n        elif isinstance(images, np.ndarray):\n            images = [images]\n            one_image = True\n        else:\n            raise Exception(f\"Input must contain np.ndarray, \"\n                            f\"tuple or list, found {type(images)}.\")\n\n        images = self.transforms(images)\n        pred = predict(images, self.model, self.tokenizer, self.device)\n\n        if one_image:\n            return pred[0]\n        else:\n            return pred","metadata":{"execution":{"iopub.status.busy":"2022-03-02T10:47:54.432088Z","iopub.execute_input":"2022-03-02T10:47:54.432569Z","iopub.status.idle":"2022-03-02T10:47:54.443960Z","shell.execute_reply.started":"2022-03-02T10:47:54.432531Z","shell.execute_reply":"2022-03-02T10:47:54.443116Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Применим модель","metadata":{}},{"cell_type":"code","source":"# Конфиг для модели OCR\n\nconfig_json = {\n    \"alphabet\": '!\"%\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№',\n    \"image\": {\n        \"width\": 256,\n        \"height\": 64\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-02T10:47:56.026106Z","iopub.execute_input":"2022-03-02T10:47:56.026718Z","iopub.status.idle":"2022-03-02T10:47:56.031527Z","shell.execute_reply.started":"2022-03-02T10:47:56.026675Z","shell.execute_reply":"2022-03-02T10:47:56.030360Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"ocr_predictor = OcrPredictor(\n    model_path='../input/tryyyy/model-base_Tema1.ckpt',\n    config=config_json,\n    device = 'cuda'\n)\n\nimg = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n\nimg = cv2.imread(img)\npred_text = ocr_predictor(img)\nprint(pred_text)\nplt.figure(figsize=(20, 20))\nplt.imshow(img)\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T10:47:57.520143Z","iopub.execute_input":"2022-03-02T10:47:57.520440Z","iopub.status.idle":"2022-03-02T10:47:58.039240Z","shell.execute_reply.started":"2022-03-02T10:47:57.520398Z","shell.execute_reply":"2022-03-02T10:47:58.038296Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"RuntimeError: Error(s) in loading state_dict for CRNN:\n\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([151, 256]) from checkpoint, the shape in current model is torch.Size([150, 256]).\n\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([151]) from checkpoint, the shape in current model is torch.Size([150]).","metadata":{}},{"cell_type":"code","source":"#['state_dict']\ntorch.load('../input/tryyyy/model-base_Tema1.ckpt')\nprint(type(temp))","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:29:52.826134Z","iopub.execute_input":"2022-03-02T09:29:52.826796Z","iopub.status.idle":"2022-03-02T09:29:52.904740Z","shell.execute_reply.started":"2022-03-02T09:29:52.826705Z","shell.execute_reply":"2022-03-02T09:29:52.903768Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"ckpt = torch.load(\"../input/tryyyy/model-base_Tema1.ckpt\")\n\nfor key in ckpt:\n    print(key)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T10:48:09.895681Z","iopub.execute_input":"2022-03-02T10:48:09.896344Z","iopub.status.idle":"2022-03-02T10:48:09.969628Z","shell.execute_reply.started":"2022-03-02T10:48:09.896306Z","shell.execute_reply":"2022-03-02T10:48:09.964494Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageFont, ImageDraw, Image\n\ndef get_image_visualization(img, pred_data, fontpath, font_koef=50):\n    h, w = img.shape[:2]\n    font = ImageFont.truetype(fontpath, int(h/font_koef))\n    empty_img = Image.new('RGB', (w, h), (255, 255, 255))\n    draw = ImageDraw.Draw(empty_img)\n\n    for prediction in pred_data['predictions']:\n        polygon = prediction['polygon']\n        pred_text = prediction['text']\n        cv2.drawContours(img, np.array([polygon]), -1, (0, 255, 0), 2)\n        x, y, w, h = cv2.boundingRect(np.array([polygon]))\n        draw.text((x, y), pred_text, fill=0, font=font)\n\n    vis_img = np.array(empty_img)\n    vis = np.concatenate((img, vis_img), axis=1)\n    return vis\n\n\n\ndef crop_img_by_polygon(img, polygon):\n    # https://stackoverflow.com/questions/48301186/cropping-concave-polygon-from-image-using-opencv-python\n    pts = np.array(polygon)\n    rect = cv2.boundingRect(pts)\n    x,y,w,h = rect\n    croped = img[y:y+h, x:x+w].copy()\n    pts = pts - pts.min(axis=0)\n    mask = np.zeros(croped.shape[:2], np.uint8)\n    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n    dst = cv2.bitwise_and(croped, croped, mask=mask)\n    return dst\n\n\n\nclass PiepleinePredictor:\n    def __init__(self, segm_model_path, ocr_model_path, ocr_config):\n        model_path = \"../input/tryyyy/weights/model-segmentation.pth\"\n        model = getattr(smp, config[\"model\"][\"name\"])(**config[\"model\"][\"parameters\"])\n        model_state = torch.load(model_path, map_location=torch.device('cpu'))[\"model_state\"]\n        model.load_state_dict(model_state)\n        model.eval()\n\n        self.segm_predictor = Predictor(model=model)\n        self.ocr_predictor = OcrPredictor(\n            model_path='../input/tryyyy/weights/model-ocr.ckpt',\n            config=config_json)\n        \n    def __call__(self, img):\n        output = {'predictions': []}\n        contours = self.segm_predictor(img)\n        for contour in contours:\n            if contour is not None:\n                crop = crop_img_by_polygon(img, contour)\n                print(crop)\n                pred_text = self.ocr_predictor(crop)\n                output['predictions'].append(\n                    {\n                        'polygon': [[int(i[0][0]), int(i[0][1])] for i in contour],\n                        'text': pred_text\n                    }\n                )\n        return output\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:43:05.119124Z","iopub.execute_input":"2022-03-02T05:43:05.119527Z","iopub.status.idle":"2022-03-02T05:43:05.137616Z","shell.execute_reply.started":"2022-03-02T05:43:05.119495Z","shell.execute_reply":"2022-03-02T05:43:05.13665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npipeline_predictor = PiepleinePredictor(\n    segm_model_path='../input/tryyyy/weights/model-segmentation.pth',\n    ocr_model_path='../input/tryyyy/weights/model-ocr.ckpt',\n    ocr_config=config_json\n)\n    \nimage_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n\nimg = cv2.imread(image_path)\noutput = pipeline_predictor(img)\n\nvis = get_image_visualization(img, output, 'font.otf')\n\nplt.figure(figsize=(20, 20))\nplt.imshow(vis)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:40:03.741188Z","iopub.execute_input":"2022-03-02T05:40:03.741642Z","iopub.status.idle":"2022-03-02T05:40:04.810511Z","shell.execute_reply.started":"2022-03-02T05:40:03.741607Z","shell.execute_reply":"2022-03-02T05:40:04.809592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RuntimeError: Error(s) in loading state_dict for CRNN:\n\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([114, 256]) from checkpoint, the shape in current model is torch.Size([151, 256]).\n\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([114]) from checkpoint, the shape in current model is torch.Size([151]).","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:43:40.70429Z","iopub.execute_input":"2022-03-02T05:43:40.704616Z","iopub.status.idle":"2022-03-02T05:43:40.718834Z","shell.execute_reply.started":"2022-03-02T05:43:40.704568Z","shell.execute_reply":"2022-03-02T05:43:40.717558Z"}}},{"cell_type":"markdown","source":"","metadata":{}}]}