{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Объединяем сегментацию и распознавание в одно работающее целое","metadata":{}},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:09.156170Z","iopub.execute_input":"2022-03-03T02:27:09.156558Z","iopub.status.idle":"2022-03-03T02:27:22.338802Z","shell.execute_reply.started":"2022-03-03T02:27:09.156459Z","shell.execute_reply":"2022-03-03T02:27:22.337992Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import sys\nimport os\nimport numpy as np\nimport random\n\nfrom tqdm import tqdm\nimport cv2\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport segmentation_models_pytorch as smp\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:22.340894Z","iopub.execute_input":"2022-03-03T02:27:22.341141Z","iopub.status.idle":"2022-03-03T02:27:28.372541Z","shell.execute_reply.started":"2022-03-03T02:27:22.341114Z","shell.execute_reply":"2022-03-03T02:27:28.371686Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"model\": {\n        \"name\": \"Unet\",\n        \"parameters\": {\n            \"encoder_name\": \"resnet34\",\n            \"encoder_weights\": None,\n            \"in_channels\": 3,\n            \"classes\": 1,\n        }\n    },\n    \"seed\": 42,\n    \"transforms\": A.Compose([\n        A.Resize(width=512, height=512),\n        A.Normalize(),\n        ToTensorV2(),\n    ]),\n\n    \"post-transforms\": A.Resize(width=3000, height=3000),\n    \"threshold\": 0.7,\n}\n\n#'../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:28.373960Z","iopub.execute_input":"2022-03-03T02:27:28.374191Z","iopub.status.idle":"2022-03-03T02:27:28.380891Z","shell.execute_reply.started":"2022-03-03T02:27:28.374160Z","shell.execute_reply":"2022-03-03T02:27:28.378773Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\"\"\"model_state = torch.load_state_dict(torch.load('../input/tryyyy/weights/model-segmentation.pth'))\nprint(model_state)\n\n\"\"\"\nmodel_path = \"../input/tryyyy/weights/model-segmentation.pth\"\nmodel = getattr(smp, config[\"model\"][\"name\"])(**config[\"model\"][\"parameters\"])\nmodel_state = torch.load(model_path, map_location=torch.device('cpu'))[\"model_state\"]\n# print(model_state.keys())","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:28.383023Z","iopub.execute_input":"2022-03-03T02:27:28.383594Z","iopub.status.idle":"2022-03-03T02:27:31.776130Z","shell.execute_reply.started":"2022-03-03T02:27:28.383557Z","shell.execute_reply":"2022-03-03T02:27:31.775366Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#A.transforms.CLAHE\n\n\nimport albumentations as A\n\ntransform = A.Compose([\n    A.transforms.CLAHE(clip_limit=(1.0, 2.0))], p=1)\n\nimage = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\nimage = cv2.imread(image)\nplt.figure(figsize=(15, 15))\nplt.imshow(image)\nplt.show()\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Augment an image\ntransformed = transform(image=image)\ntransformed_image = transformed[\"image\"]\n\n\n\n\nplt.figure(figsize=(15, 15))\nplt.imshow(transformed_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:31.778452Z","iopub.execute_input":"2022-03-03T02:27:31.778716Z","iopub.status.idle":"2022-03-03T02:27:35.583048Z","shell.execute_reply.started":"2022-03-03T02:27:31.778681Z","shell.execute_reply":"2022-03-03T02:27:35.580051Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Predictor:\n    def __init__(self, model, device=\"cpu\", threshold=0.7):\n        self.model = model\n        self.device = torch.device(device)\n        self.threshold = threshold\n\n    def __call__(self, loader):\n        predictions = []\n\n        self.model.to(self.device)\n        with torch.no_grad():\n            for batch in tqdm(loader, position=0, leave=True, desc=\"Predicting: \"):\n                inputs = batch.to(self.device)\n                batch_predictions = self.model(inputs)\n                batch_predictions = batch_predictions.to(\"cpu\").detach()\n                batch_predictions = torch.where(batch_predictions > self.threshold, 1, 0)\n                batch_predictions = batch_predictions.permute(0, 2, 3, 1).numpy()\n                predictions.extend(batch_predictions)\n\n        predictions = np.array(predictions)\n        return predictions\n\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, pathes, masks=None, transforms=None):\n        self.pathes = pathes\n        self.masks = masks\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.pathes)\n\n    def __getitem__(self, index):\n        image_path = self.pathes[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.masks:\n            mask = self.masks[index].astype(np.int8)\n\n        if self.transforms:\n            if self.masks:\n                transformed = self.transforms(image=image, mask=mask)\n                image = transformed[\"image\"]\n                mask = transformed[\"mask\"]\n            else:\n                transformed = self.transforms(image=image)\n                image = transformed[\"image\"]\n\n        if self.masks:\n            return image, mask\n\n        return image\n\ndef main():\n    #test_image_path, output_path = sys.argv[1:]\n    test_image_path = '../input/tryyyy/data/data/train_segmentation/images/0_1_eng.jpg'\n    output_path = './try'\n    #test_images_filenames = os.listdir(test_image_path)\n    test_images_pathes = [test_image_path]\n    #test_images_pathes = [os.path.join(test_image_path, file) for file in test_images_filenames]\n    shapes = np.array([np.asarray(cv2.imread(path)).shape[1::-1] for path in test_images_pathes])\n\n    print(shapes)\n\n\n    dataset = SegmentationDataset(pathes=test_images_pathes, transforms=config[\"transforms\"])\n    loader = DataLoader(dataset=dataset,\n                        batch_size=9,\n                        shuffle=False,\n                        num_workers=0,\n                        pin_memory=True,\n                        drop_last=False)\n\n    print(f\"Loaded dataset with: {len(dataset)} samples\")\n\n    model_path = \"../input/tryyyy/weights/model-segmentation.pth\"\n    model = getattr(smp, config[\"model\"][\"name\"])(**config[\"model\"][\"parameters\"])\n    model_state = torch.load(model_path, map_location=torch.device('cpu'))[\"model_state\"]\n    model.load_state_dict(model_state)\n    print(f\"Loaded {model_path}\")\n    model.eval()\n\n    predictor = Predictor(model=model, device=\"cpu\", threshold=config[\"threshold\"])\n    predictions = predictor(loader)\n\n    resized_predictions = []\n    for prediction, shape in zip(predictions, shapes):\n        prediction = prediction.astype(np.int8)\n        resized_prediction = cv2.resize(prediction,shape, interpolation=cv2.INTER_NEAREST)\n        resized_predictions.append(resized_prediction)\n\n    resized_predictions = np.array(resized_predictions)\n    results = {k: v for k, v in zip(test_images_pathes, resized_predictions)}\n    np.savez_compressed(output_path, **results)\n\n    print(f\"Saved results: {output_path}\")\n    return results\n\n#хочу бви (-_-)\nif __name__ == \"__main__\":\n    image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    img = cv2.imread(image_path)\n    mask = main()\n    for contour in mask:\n        cv2.drawContours(img, np.array([contour]), -1, (0, 255, 0), 2)\n\n    plt.figure(figsize=(15, 15))\n    plt.imshow(img)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:29:52.791774Z","iopub.execute_input":"2022-03-03T02:29:52.792083Z","iopub.status.idle":"2022-03-03T02:29:54.926591Z","shell.execute_reply.started":"2022-03-03T02:29:52.792049Z","shell.execute_reply":"2022-03-03T02:29:54.924858Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_contours_from_mask(mask, min_area=20):\n    mask = torch.from_numpy(mask)\n    contours, hierarchy = cv2.findContours(mask.astype(np.uint8),\n                                           cv2.RETR_LIST,\n                                           cv2.CHAIN_APPROX_SIMPLE)\n    contour_list = []\n    for contour in contours:\n        if cv2.contourArea(contour) >= min_area:\n            contour_list.append(contour)\n    return contour_list\n\n\nif __name__ == \"__main__\":\n    image_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n    img = cv2.imread(image_path)\n    mask = main()\n    print(mask)\n    contours = get_contours_from_mask(mask[image_path])\n    img = cv2.imread(image_path)\n\n    for contour in contours:\n        cv2.drawContours(img, np.array([contour]), -1, (0, 255, 0), 2)\n\n    plt.figure(figsize=(15, 15))\n    plt.imshow(img)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:30:07.426285Z","iopub.execute_input":"2022-03-03T02:30:07.426542Z","iopub.status.idle":"2022-03-03T02:30:09.241936Z","shell.execute_reply.started":"2022-03-03T02:30:07.426512Z","shell.execute_reply":"2022-03-03T02:30:09.240964Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nimage_path = '../input/tryyyy/data/data/train_segmentation/images/0_1_eng.jpg'\nimg = cv2.imread(image_path)\n\n    \nfig = plt.figure(figsize=(20, 20))\n\n\nax1 = fig.add_subplot(1, 3, 2)\nax1.imshow(mask[image_path].squeeze())\nax1.set_title(\"Ground Truth\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:30:22.442325Z","iopub.execute_input":"2022-03-03T02:30:22.442757Z","iopub.status.idle":"2022-03-03T02:30:24.457794Z","shell.execute_reply.started":"2022-03-03T02:30:22.442711Z","shell.execute_reply":"2022-03-03T02:30:24.457104Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def crop_img_by_polygon(img, polygon):\n    # https://stackoverflow.com/questions/48301186/cropping-concave-polygon-from-image-using-opencv-python\n    pts = np.array(polygon)\n    rect = cv2.boundingRect(pts)\n    x,y,w,h = rect\n    croped = img[y:y+h, x:x+w].copy()\n    pts = pts - pts.min(axis=0)\n    mask = np.zeros(croped.shape[:2], np.uint8)\n    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n    dst = cv2.bitwise_and(croped, croped, mask=mask)\n    return dst\n\nimage_path = '../input/tryyyy/data/data/train_segmentation/images/0_1_eng.jpg'\nimg = cv2.imread(image_path)\n\nfor contour in contours:\n    if contour is not None:\n        crop = crop_img_by_polygon(img, contour)\n        #print(crop)\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:30:28.211466Z","iopub.execute_input":"2022-03-03T02:30:28.212165Z","iopub.status.idle":"2022-03-03T02:30:28.364219Z","shell.execute_reply.started":"2022-03-03T02:30:28.212128Z","shell.execute_reply":"2022-03-03T02:30:28.363324Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Щас будем творить вторую часть OCR","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\n\nimport numpy as np\nimport cv2\nimport os\nimport sys\nimport json","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:30:53.076804Z","iopub.execute_input":"2022-03-03T02:30:53.077472Z","iopub.status.idle":"2022-03-03T02:30:53.081908Z","shell.execute_reply.started":"2022-03-03T02:30:53.077430Z","shell.execute_reply":"2022-03-03T02:30:53.080694Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"OOV_TOKEN = '<OOV>'\nCTC_BLANK = '<BLANK>'\n\n\ndef get_char_map(alphabet):\n    \"\"\"Make from string alphabet character2int dict.\n    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n    char_map[CTC_BLANK] = 0\n    char_map[OOV_TOKEN] = 1\n    return char_map\n\n\nclass Tokenizer:\n    \"\"\"Class for encoding and decoding string word to sequence of int\n    (and vice versa) using alphabet.\"\"\"\n\n    def __init__(self, alphabet):\n        self.char_map = get_char_map(alphabet)\n        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n\n    def encode(self, word_list):\n        \"\"\"Returns a list of encoded words (int).\"\"\"\n        enc_words = []\n        for word in word_list:\n            enc_words.append(\n                [self.char_map[char] if char in self.char_map\n                 else self.char_map[OOV_TOKEN]\n                 for char in word]\n            )\n        return enc_words\n\n    def get_num_chars(self):\n        return 150#len(self.char_map)\n\n    def decode(self, enc_word_list):\n        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n        repeating characters. Also skip out of vocabulary token.\"\"\"\n        dec_words = []\n        for word in enc_word_list:\n            word_chars = ''\n            for idx, char_enc in enumerate(word):\n                # skip if blank symbol, oov token or repeated characters\n                if (\n                    char_enc != self.char_map[OOV_TOKEN]\n                    and char_enc != self.char_map[CTC_BLANK]\n                    # idx > 0 to avoid selecting [-1] item\n                    and not (idx > 0 and char_enc == word[idx - 1])\n                ):\n                    word_chars += self.rev_char_map[char_enc]\n            dec_words.append(word_chars)\n        return dec_words","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:30:54.332711Z","iopub.execute_input":"2022-03-03T02:30:54.333158Z","iopub.status.idle":"2022-03-03T02:30:54.344902Z","shell.execute_reply.started":"2022-03-03T02:30:54.333122Z","shell.execute_reply":"2022-03-03T02:30:54.343387Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Базовые трансформы модели\n\nЗдесь мы задаем базовые трансформы для инференса OCR","metadata":{}},{"cell_type":"code","source":"from albumentations.pytorch.transforms import ToTensorV2\n\nclass Normalize:\n    def __call__(self, img):\n        img = img.astype(np.float32) / 255\n        return img\n\n\nclass MoveChannels:\n    \"\"\"Move the channel axis to the zero position as required in pytorch.\"\"\"\n\n    def __init__(self, to_channels_first=True):\n        self.to_channels_first = to_channels_first\n\n    def __call__(self, image):\n        if self.to_channels_first:\n            return np.moveaxis(image, -1, 0)\n        else:\n            return np.moveaxis(image, 0, -1)\n\n\nclass ImageResize:\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n\n    def __call__(self, image):\n        image = cv2.resize(image, (self.width, self.height),\n                           interpolation=cv2.INTER_LINEAR)\n        return image\n\n\ndef get_val_transforms(height, width):\n    transforms = torchvision.transforms.Compose([\n        ImageResize(height, width),\n        MoveChannels(to_channels_first=True),\n        Normalize(),\n        ToTensorV2(),\n    ])\n    return transforms","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:30:56.526501Z","iopub.execute_input":"2022-03-03T02:30:56.527286Z","iopub.status.idle":"2022-03-03T02:30:56.537669Z","shell.execute_reply.started":"2022-03-03T02:30:56.527237Z","shell.execute_reply":"2022-03-03T02:30:56.536871Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### 2.3. Здесь определяем саму модель - CRNN","metadata":{}},{"cell_type":"code","source":"#torch.load('../input/tryyyy/model-crnn_Tema2.ckpt')","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:41.231532Z","iopub.status.idle":"2022-03-03T02:27:41.232391Z","shell.execute_reply.started":"2022-03-03T02:27:41.232160Z","shell.execute_reply":"2022-03-03T02:27:41.232185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4. Определяем класс для использования OCR-модели на инференсе","metadata":{}},{"cell_type":"code","source":"# Конфиг для модели OCR\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nconfig_json = {\n    \"alphabet\": ''' !\"%\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№''',\n    \"save_dir\": \"./experiments/test\",\n    \"num_epochs\": 100,\n    \"image\": {\n        \"width\": 256,\n        \"height\": 64\n    },\n    \"train\": {\n        \"root_path\": \"../input/tryyyy/data/data/train_recognition/images\",\n        \"json_path\": \"./train_labels_splitted.json\",\n        \"batch_size\": 128\n    },\n    \"val\": {\n        \"root_path\": \"../input/tryyyy/data/data/train_recognition/images\",\n        \"json_path\": \"./val_labels_splitted.json\",\n        \"batch_size\": 128\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:57:22.287022Z","iopub.execute_input":"2022-03-03T02:57:22.287277Z","iopub.status.idle":"2022-03-03T02:57:22.293370Z","shell.execute_reply.started":"2022-03-03T02:57:22.287249Z","shell.execute_reply":"2022-03-03T02:57:22.292413Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def get_resnet34_backbone(pretrained=True):\n    m = torchvision.models.resnet34(pretrained=True)\n    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n    blocks = [input_conv, m.bn1, m.relu,\n              m.maxpool, m.layer1, m.layer2, m.layer3]\n    return nn.Sequential(*blocks)\n\n\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.15):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size, hidden_size, num_layers,\n            dropout=dropout, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return out\n\n\nclass CRNN(nn.Module):\n    def __init__(\n        self, number_class_symbols, time_feature_count=256, lstm_hidden=256,\n        lstm_len=2,\n    ):\n        super().__init__()\n        self.feature_extractor = get_resnet34_backbone(pretrained=True)\n        self.avg_pool = nn.AdaptiveAvgPool2d(\n            (time_feature_count, time_feature_count))\n        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_hidden * 2, time_feature_count),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(time_feature_count, number_class_symbols),\n        )\n\n    def forward(self, x):\n        x = self.feature_extractor(x)\n        b, c, h, w = x.size()\n        x = x.view(b, c * h, w)\n        x = self.avg_pool(x)\n        x = x.transpose(1, 2)\n        x = self.bilstm(x)\n        x = self.classifier(x)\n        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-03-03T03:00:25.712859Z","iopub.execute_input":"2022-03-03T03:00:25.713108Z","iopub.status.idle":"2022-03-03T03:00:25.725737Z","shell.execute_reply.started":"2022-03-03T03:00:25.713081Z","shell.execute_reply":"2022-03-03T03:00:25.725048Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"class InferenceTransform:\n    def __init__(self, height, width):\n        self.transforms = get_val_transforms(height, width)\n\n    def __call__(self, images):\n        transformed_images = list()\n        for image in images:\n            image = self.transforms(image=image)\n            transformed_images.append(image['image'])\n        transformed_tensor = torch.stack(transformed_images, 0)\n        return transformed_tensor\n\n\nclass OcrPredictor:\n    def __init__(self, model_path, config, device='cuda'):\n        self.tokenizer = Tokenizer(config['alphabet'])\n        self.device = torch.device(device)\n        # load model\n        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.to(self.device)\n\n        self.transforms = InferenceTransform(\n            height=config['image']['height'],\n            width=config['image']['width'],\n        )\n\n    def __call__(self, images):\n        if isinstance(images, (list, tuple)):\n            one_image = False\n        elif isinstance(images, np.ndarray):\n            images = [images]\n            one_image = True\n        else:\n            raise Exception(f\"Input must contain np.ndarray, \"\n                            f\"tuple or list, found {type(images)}.\")\n\n        images = self.transforms(images)\n        pred = predict(images, self.model, self.tokenizer, self.device)\n\n        if one_image:\n            return pred[0]\n        else:\n            return pred","metadata":{"execution":{"iopub.status.busy":"2022-03-03T03:00:28.998351Z","iopub.execute_input":"2022-03-03T03:00:28.999229Z","iopub.status.idle":"2022-03-03T03:00:29.010651Z","shell.execute_reply.started":"2022-03-03T03:00:28.999189Z","shell.execute_reply":"2022-03-03T03:00:29.009748Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"predictor = OcrPredictor(\n    model_path='../input/weight/model-7-0.2085.ckpt',\n    config=config_json\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T03:00:40.366280Z","iopub.execute_input":"2022-03-03T03:00:40.366536Z","iopub.status.idle":"2022-03-03T03:00:40.923593Z","shell.execute_reply.started":"2022-03-03T03:00:40.366507Z","shell.execute_reply":"2022-03-03T03:00:40.922505Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"\n\npred_json = {}\n\n# img_path = \"../input/tryyyy/data/data/train_recognition/images/0.png\"\n# img = cv2.imread(img_path)\n# pred = predictor(img)\n\n# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n# plt.imshow(img)\n# plt.show()\n\n# print('Prediction: ', pred)\n# print('True: ', val_img[1])\n\n\nprint_images = True\nfor val_img in val_data_splitted[20:25]:\n    img = cv2.imread(f'../input/tryyyy/data/data/train_recognition/images/{val_img[0]}')\n    \n    pred = predictor(img)\n    pred_json[val_img[0]] = pred\n\n    if print_images:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.imshow(img)\n        plt.show()\n        print('Prediction: ', predictor(img))\n        print('True: ', val_img[1])","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:53:32.009536Z","iopub.execute_input":"2022-03-03T02:53:32.009945Z","iopub.status.idle":"2022-03-03T02:53:32.563873Z","shell.execute_reply.started":"2022-03-03T02:53:32.009912Z","shell.execute_reply":"2022-03-03T02:53:32.562908Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### Применим модель","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntrain_csv = pd.read_csv('../input/tryyyy/data/data/train_recognition/labels.csv')\ntrain_csv = train_csv.sample(frac = 1)\n\ntrain_data = dict(train_csv[['file_name','text']].values)\n\ntrain_data = [(k, v) for k, v in train_data.items()]\nprint('train len', len(train_data))\n\nsplit_coef = 0.75\ntrain_len = int(len(train_data)*split_coef)\n\ntrain_data_splitted = train_data[:train_len]\nval_data_splitted = train_data[train_len:]\n\nprint('train len after split', len(train_data_splitted))\nprint('val len after split', len(val_data_splitted))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:53:28.866409Z","iopub.execute_input":"2022-03-03T02:53:28.866688Z","iopub.status.idle":"2022-03-03T02:53:29.347432Z","shell.execute_reply.started":"2022-03-03T02:53:28.866658Z","shell.execute_reply":"2022-03-03T02:53:29.346692Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"with open('./train_labels_splitted.json', 'w') as f:\n    json.dump(dict(train_data_splitted), f)\n    \nwith open('./val_labels_splitted.json', 'w') as f:\n    json.dump(dict(val_data_splitted), f)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:43:09.110201Z","iopub.execute_input":"2022-03-03T02:43:09.110802Z","iopub.status.idle":"2022-03-03T02:43:09.442602Z","shell.execute_reply.started":"2022-03-03T02:43:09.110762Z","shell.execute_reply":"2022-03-03T02:43:09.441745Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"RuntimeError: Error(s) in loading state_dict for CRNN:\n\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([151, 256]) from checkpoint, the shape in current model is torch.Size([150, 256]).\n\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([151]) from checkpoint, the shape in current model is torch.Size([150]).","metadata":{}},{"cell_type":"code","source":"#['state_dict']\ntemp = torch.load('../input/tryyyy/model-crnn_Tema2.ckpt')\nprint(list(temp.values())[0].shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:41.239759Z","iopub.status.idle":"2022-03-03T02:27:41.240630Z","shell.execute_reply.started":"2022-03-03T02:27:41.240369Z","shell.execute_reply":"2022-03-03T02:27:41.240394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt = torch.load(\"../input/tryyyy/model-crnn_Tema2.ckpt\")\n\n\"\"\"for key in ckpt:\n    print(key)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:41.241744Z","iopub.status.idle":"2022-03-03T02:27:41.242272Z","shell.execute_reply.started":"2022-03-03T02:27:41.242037Z","shell.execute_reply":"2022-03-03T02:27:41.242062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageFont, ImageDraw, Image\n\ndef get_image_visualization(img, pred_data, fontpath, font_koef=50):\n    h, w = img.shape[:2]\n    font = ImageFont.truetype(fontpath, int(h/font_koef))\n    empty_img = Image.new('RGB', (w, h), (255, 255, 255))\n    draw = ImageDraw.Draw(empty_img)\n\n    for prediction in pred_data['predictions']:\n        polygon = prediction['polygon']\n        pred_text = prediction['text']\n        cv2.drawContours(img, np.array([polygon]), -1, (0, 255, 0), 2)\n        x, y, w, h = cv2.boundingRect(np.array([polygon]))\n        draw.text((x, y), pred_text, fill=0, font=font)\n\n    vis_img = np.array(empty_img)\n    vis = np.concatenate((img, vis_img), axis=1)\n    return vis\n\n\n\ndef crop_img_by_polygon(img, polygon):\n    # https://stackoverflow.com/questions/48301186/cropping-concave-polygon-from-image-using-opencv-python\n    pts = np.array(polygon)\n    rect = cv2.boundingRect(pts)\n    x,y,w,h = rect\n    croped = img[y:y+h, x:x+w].copy()\n    pts = pts - pts.min(axis=0)\n    mask = np.zeros(croped.shape[:2], np.uint8)\n    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n    dst = cv2.bitwise_and(croped, croped, mask=mask)\n    return dst\n\n\n\nclass PiepleinePredictor:\n    def __init__(self, segm_model_path, ocr_model_path, ocr_config):\n        model_path = \"../input/tryyyy/weights/model-segmentation.pth\"\n        model = getattr(smp, config[\"model\"][\"name\"])(**config[\"model\"][\"parameters\"])\n        model_state = torch.load(model_path, map_location=torch.device('cpu'))[\"model_state\"]\n        model.load_state_dict(model_state)\n        model.eval()\n\n        self.segm_predictor = Predictor(model=model)\n        self.ocr_predictor = OcrPredictor(\n            model_path='../input/tryyyy/weights/model-ocr.ckpt',\n            config=config_json)\n        \n    def __call__(self, img):\n        output = {'predictions': []}\n        contours = self.segm_predictor(img)\n        for contour in contours:\n            if contour is not None:\n                crop = crop_img_by_polygon(img, contour)\n                print(crop)\n                pred_text = self.ocr_predictor(crop)\n                output['predictions'].append(\n                    {\n                        'polygon': [[int(i[0][0]), int(i[0][1])] for i in contour],\n                        'text': pred_text\n                    }\n                )\n        return output\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:41.243958Z","iopub.status.idle":"2022-03-03T02:27:41.244555Z","shell.execute_reply.started":"2022-03-03T02:27:41.244314Z","shell.execute_reply":"2022-03-03T02:27:41.244339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npipeline_predictor = PiepleinePredictor(\n    segm_model_path='../input/tryyyy/weights/model-segmentation.pth',\n    ocr_model_path='../input/tryyyy/weights/model-ocr.ckpt',\n    ocr_config=config_json\n)\n    \nimage_path = '../input/tryyyy/data/data/train_segmentation/images/0_0_eng.jpg'\n\nimg = cv2.imread(image_path)\noutput = pipeline_predictor(img)\n\nvis = get_image_visualization(img, output, 'font.otf')\n\nplt.figure(figsize=(20, 20))\nplt.imshow(vis)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T02:27:41.245922Z","iopub.status.idle":"2022-03-03T02:27:41.246771Z","shell.execute_reply.started":"2022-03-03T02:27:41.246521Z","shell.execute_reply":"2022-03-03T02:27:41.246545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RuntimeError: Error(s) in loading state_dict for CRNN:\n\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([114, 256]) from checkpoint, the shape in current model is torch.Size([151, 256]).\n\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([114]) from checkpoint, the shape in current model is torch.Size([151]).","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:43:40.70429Z","iopub.execute_input":"2022-03-02T05:43:40.704616Z","iopub.status.idle":"2022-03-02T05:43:40.718834Z","shell.execute_reply.started":"2022-03-02T05:43:40.704568Z","shell.execute_reply":"2022-03-02T05:43:40.717558Z"}}},{"cell_type":"markdown","source":"","metadata":{}}]}